services:
  postgres:
    image: postgres:17 # TODO: ensure compatibility with our GCP PostgreSQL
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_DB
      - POSTGRES_HOST
      - POSTGRES_PORT
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  mlflow:
    build:
      dockerfile: Dockerfile.mlflow
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI}
      MLFLOW_ARTIFACT_DESTINATION: ${MLFLOW_ARTIFACT_DESTINATION}
      # if not provided via volume below, GS will not work as artifact store
      GOOGLE_APPLICATION_CREDENTIALS: /creds/gcp-key.json
      # if not provided via volume below, AWS S3 will not work as artifact store
      AWS_SHARED_CREDENTIALS_FILE: /creds/aws-credentials
    depends_on:
      - postgres
    # grab backend from .env, pass artifact root, if provided, otherwise local storage of artifacts
    command: >
      mlflow server
        --backend-store-uri postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
        --artifacts-destination ${MLFLOW_ARTIFACT_DESTINATION}
        --serve-artifacts
        --host 0.0.0.0
        --port 8080
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      # Volume only needed for local storage of artifacts
      - ./mlruns:/mlruns
      # Volume only needed for GS storage artifacts
      - ${GOOGLE_CREDENTIALS_PATH:-/dev/null}:/creds/gcp-key.json:ro
      # Volume only needed for AWS S3 storage artifacts
      - ${AWS_CREDENTIALS_PATH:-/dev/null}:/creds/aws-credentials:ro
  
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile.jupyter
      args:
        USE_PRIVATE_MODELBENCH: ${USE_PRIVATE_MODELBENCH}
    environment:
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI}
      MLFLOW_TRACKING_USERNAME: ${MLFLOW_TRACKING_USERNAME}
      MLFLOW_TRACKING_PASSWORD: ${MLFLOW_TRACKING_PASSWORD}
      USE_PRIVATE_MODELBENCH: ${USE_PRIVATE_MODELBENCH}
      JUPYTER_TOKEN: ${JUPYTER_TOKEN}
      GIT_PYTHON_REFRESH: ${GIT_PYTHON_REFRESH}
      VLLM_API_KEY: ${VLLM_API_KEY}
      # Below env needed for dvc (via git) support (backed by GCP)
      # SSH_AUTH_SOCK: /ssh-agent
      # GOOGLE_APPLICATION_CREDENTIALS: /creds/gcp-key.json
    ports:
      - "8888:8888"
    volumes:
      - ./flightpaths:/app/flightpaths
      # Volume not needed if not using modelplane-flights for sharing notebooks
      - ../modelplane-flights:/app/flightpaths/flights
      # Volume not needed if using cloud storage for artifacts
      - ./mlruns:/mlruns
      # Below needed for dvc (via git) support (backed by GCP)
      # - ${SSH_AUTH_SOCK:-/dev/null}:/ssh-agent
      # - ${GOOGLE_CREDENTIALS_PATH:-/dev/null}:/creds/gcp-key.json:ro

  # Runs a dummy docker container to mock a vLLM server
  vllm:
    build:
      context: .
      dockerfile: Dockerfile.mockvllm
    environment:
      VLLM_MODEL: mlc/not-real-model
      VLLM_HOST: 0.0.0.0
      VLLM_PORT: 8001
      VLLM_API_KEY: ${VLLM_API_KEY}
    ports:
      - "8001:8001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 10s
      timeout: 5s
      retries: 10

volumes:
  pgdata:
