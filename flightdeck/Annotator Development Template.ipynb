{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab195250-6a0f-4176-a09d-3696d911203d",
   "metadata": {},
   "source": [
    "# Annotator/Evaluator Development Template\n",
    "\n",
    "Use this template to iterate on a new annotator. Adjust the settings as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d5865-2cd7-4b81-a588-dfec27727643",
   "metadata": {},
   "source": [
    "## Settings and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e837c-05e9-4e62-916d-9884bb47839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelplane.runways import responder, annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f9c85-d146-4119-854b-e009235aa4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sut_id = \"llama-3.1-8b-instruct-turbo-together\"\n",
    "experiment = \"experiment_name\"\n",
    "prompts = \"airr_official_1.0_demo_en_us_prompt_set_release_reduced.csv\"\n",
    "cache_dir = None\n",
    "n_jobs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e0529-4e51-45ae-b2ab-313915881f98",
   "metadata": {},
   "source": [
    "## Define a new annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89674a6d-b2c5-42a3-9a0c-927101126877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from modelgauge.annotator import CompletionAnnotator\n",
    "from modelgauge.annotator_registry import ANNOTATORS\n",
    "from modelgauge.annotators.llama_guard_annotator import LlamaGuardAnnotation\n",
    "from modelgauge.single_turn_prompt_response import TestItem\n",
    "from modelgauge.sut import SUTResponse\n",
    "from modelgauge.tests.safe_v1 import BaseSafeTestVersion1\n",
    "\n",
    "class AnnotationRequest(BaseModel):\n",
    "    test_item: TestItem\n",
    "    response: SUTResponse\n",
    "\n",
    "\n",
    "class NewAnnotator(CompletionAnnotator[LlamaGuardAnnotation]):\n",
    "    \"\"\"Simple random implementation for demonstration. Uses LlamaGuardAnnotation\n",
    "    for consistency with AILuminate benchmarks.\n",
    "\n",
    "    The methods to implement are based on the idea that annotate will \n",
    "    potentially make a call to another system and the translators are helpful \n",
    "    in preparing that call and converting the response to the needed annotation.\n",
    "    \"\"\"\n",
    "    # Probability of unsafe per category\n",
    "    PROB_UNSAFE = 0.01\n",
    "\n",
    "    def translate_request(self, test_item: TestItem, response: SUTResponse):\n",
    "        return AnnotationRequest(test_item=test_item, response=response)\n",
    "\n",
    "    def annotate(self, annotation_request: AnnotationRequest) -> LlamaGuardAnnotation:\n",
    "        unsafe_categories = []\n",
    "        for category in BaseSafeTestVersion1.hazards:\n",
    "            if random.random() < self.PROB_UNSAFE:\n",
    "                unsafe_categories.append(category)\n",
    "        return LlamaGuardAnnotation(\n",
    "            is_safe=not unsafe_categories,\n",
    "            is_safe_logprob=random.random(),\n",
    "            violation_categories=unsafe_categories,\n",
    "        )\n",
    "\n",
    "    def translate_response(self, request: AnnotationRequest, response: LlamaGuardAnnotation) -> LlamaGuardAnnotation:\n",
    "        return response\n",
    "\n",
    "\n",
    "annotator_id = \"new_annotator\"\n",
    "ANNOTATORS.register(NewAnnotator, annotator_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17760cd3-23fe-4c79-8882-475d8d7096ea",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "Save this run_id to avoid having to re-run the model later. The results are saved as an artifact in mlflow.\n",
    "\n",
    "NOTE: you may need to replace http://mlflow:8080 with http://localhost:8080 in the generated mlflow links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d76d5-a3e1-4cc0-aeff-e71b6ff64825",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = responder.respond(\n",
    "    sut_id=sut_id,\n",
    "    experiment=experiment,\n",
    "    prompts=prompts,\n",
    "    cache_dir=cache_dir,\n",
    "    n_jobs=n_jobs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a8a85-c171-4d11-b094-cd617b14b6ed",
   "metadata": {},
   "source": [
    "## Annotate the model\n",
    "\n",
    "The evaluation will be available in mlflow, and the artifact will be saved with that mlflow run for inspection of the output jsonl.\n",
    "\n",
    "NOTE: you may need to replace http://mlflow:8080 with http://localhost:8080 in the generated mlflow links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06632c4d-90bd-4c2d-9c36-84e59dd8f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator.annotate(\n",
    "    annotator_id=annotator_id,\n",
    "    experiment=experiment,\n",
    "    response_run_id=run_id,\n",
    "    cache_dir=cache_dir,\n",
    "    n_jobs=n_jobs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
