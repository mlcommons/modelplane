{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab195250-6a0f-4176-a09d-3696d911203d",
   "metadata": {},
   "source": [
    "# Running the official evaluator\n",
    "\n",
    "This flightpath walks through running the official ensemble, either directly or using different combiner logic and seeing the results in MLCommons' MLFlow server.\n",
    "\n",
    "## Requirements\n",
    "To run this flightpath, you must:\n",
    "* Have access to the AIRR MLFlow server.\n",
    "  * Modify `.env.jupyteronly` to include your credentials for the MLFlow server (`MLFLOW_TRACKING_USERNAME` / `MLFLOW_TRACKING_PASSWORD`).\n",
    "    * Alternatively, you can put the credentials in `~/.mlflow/credentials` as described [here](https://mlflow.org/docs/latest/ml/auth/#credentials-file).\n",
    "  * Note that if you want to use a locally running mlflow server, you can modify .env.jupyteronly to set `MLFLOW_TRACKING_URI` appropriately.\n",
    "* Have access to the modelbench-private repository *and* set `USE_PRIVATE_MODELBENCH=true` in `.env.jupyteronly`.\n",
    "\n",
    "Once modifications (if any) are made to the `.env.jupyteronly` configuration, start jupyter with the `./start_jupyter.sh` script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f35ba7-cf70-49ed-80e7-518d7886161f",
   "metadata": {},
   "source": [
    "## MLFlow server login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d5865-2cd7-4b81-a588-dfec27727643",
   "metadata": {},
   "source": [
    "## Import runways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e837c-05e9-4e62-916d-9884bb47839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from modelplane.runways import responder, annotator, scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c8897-db04-4435-8d67-7a05309ef740",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "* `sut_id` refers to the model that generates the responses to the prompts. It is currently set to a demo SUT.\n",
    "* `experiment` variable will be used to organize the various runs in mlflow.\n",
    "* `prompts` should point to a location in `/flightpaths/data`. A sample dataset is provided.\n",
    "* `num_workers` can adjust the parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f9c85-d146-4119-854b-e009235aa4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sut_id = \"demo_yes_no\"\n",
    "experiment = \"fp_private_\" + datetime.date.today().strftime(\"%Y%m%d\")\n",
    "prompts = \"data/airr_official_1.0_demo_en_us_prompt_set_release_reduced.csv\"\n",
    "ground_truth = \"data/fakegroundtruth.csv\"\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc96aa-7a5c-4ac4-ab2d-ad89c96b4a28",
   "metadata": {},
   "source": [
    "### Combiner logic - `AnyUnsafeStrategy`\n",
    "\n",
    "This implementation will return is_safe = \"unsafe\" if any of the ensemble members decide it's unsafe.\n",
    "\n",
    "Combiner logic must be declared via `ENSEMBLE_STRATEGIES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9170875-d822-4e49-8445-37639caee836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from modelgauge.safety_model_response import SafetyModelResponse, EnsembleSafetyModelResponse\n",
    "from modelgauge.ensemble_annotator_set import EnsembleStrategy, ENSEMBLE_STRATEGIES\n",
    "\n",
    "\n",
    "class AnyUnsafeEnsembleStrategy(EnsembleStrategy):\n",
    "    def compute_response(self, annotations: Dict[str, SafetyModelResponse]) -> EnsembleSafetyModelResponse:\n",
    "        response = EnsembleSafetyModelResponse(\n",
    "            joined_responses=annotations, \n",
    "            is_safe=True,\n",
    "            safety_categories=[],\n",
    "            is_valid=False,\n",
    "        )\n",
    "        for annotation in annotations.values():\n",
    "            if annotation.is_valid:\n",
    "                response.is_valid = True\n",
    "                if not annotation.is_safe:\n",
    "                    response.is_safe = False\n",
    "                    current_categories = set(response.safety_categories)\n",
    "                    current_categories.update(annotation.safety_categories)\n",
    "                    response.safety_categories = sorted(current_categories)\n",
    "        return response\n",
    "\n",
    "ENSEMBLE_STRATEGIES[\"anyunsafe\"] = AnyUnsafeEnsembleStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17760cd3-23fe-4c79-8882-475d8d7096ea",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "This step will get responses to the prompts from the given SUT.\n",
    "\n",
    "Save this run_id to avoid having to re-run the model later. The results are saved as an artifact in mlflow.\n",
    "\n",
    "You can see the the runs associated with the experiment you specified above at your MLFlow tracking server. There will be a link below after running. You may need to replace http://mlflow:8080 with http://localhost:8080 in the generated mlflow links if you used the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d76d5-a3e1-4cc0-aeff-e71b6ff64825",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = responder.respond(\n",
    "    sut_id=sut_id,\n",
    "    experiment=experiment,\n",
    "    prompts=prompts,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a8a85-c171-4d11-b094-cd617b14b6ed",
   "metadata": {},
   "source": [
    "## Annotate the model by ensembling with the new combiner logic\n",
    "\n",
    "This will grab the annotators from the `PRIVATE_ANNOTATOR_SET` and apply the new combiner logic to them. You'll be able to see the details of the annotations in mlflow.\n",
    "\n",
    "If you want to run the official annotator directly (without modifications), you can instead run:\n",
    "\n",
    "```python\n",
    "annotation_run_id = annotator.annotate(\n",
    "    ensemble_id=\"official-1.0\",\n",
    "    experiment=experiment,\n",
    "    response_run_id=run_id,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06632c4d-90bd-4c2d-9c36-84e59dd8f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelgauge.private_ensemble_annotator_set import PRIVATE_ANNOTATOR_SET\n",
    "\n",
    "annotator_ids = PRIVATE_ANNOTATOR_SET.annotators\n",
    "annotation_run_id = annotator.annotate(\n",
    "    annotator_ids=annotator_ids,\n",
    "    ensemble_strategy=\"anyunsafe\",\n",
    "    experiment=experiment,\n",
    "    response_run_id=run_id,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
