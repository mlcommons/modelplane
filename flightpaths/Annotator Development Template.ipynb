{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab195250-6a0f-4176-a09d-3696d911203d",
   "metadata": {},
   "source": [
    "# Evaluator Development Template\n",
    "\n",
    "Use this template to iterate on a new evaluator (a.k.a. annotator). \n",
    "\n",
    "The general flow is that you would initially generate some prompt responses for some system under test (SUT).\n",
    "\n",
    "Then, you would evaluate those responses with the newly defined annotator and score them. \n",
    "\n",
    "And then iterate.\n",
    "\n",
    "Sample code is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d5865-2cd7-4b81-a588-dfec27727643",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e837c-05e9-4e62-916d-9884bb47839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from modelplane.runways import responder, annotator, scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c8897-db04-4435-8d67-7a05309ef740",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "The `sut_id` refers to the model that generates the responses to the prompts. \n",
    "\n",
    "The `experiment` variable will be used to organize the various runs in mlflow.\n",
    "\n",
    "The `prompts` should point to a location in `/flightpaths/data`. A sample dataset is provided.\n",
    "\n",
    "Finally, `num_workers` can adjust the parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f9c85-d146-4119-854b-e009235aa4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sut_id = \"demo_yes_no\"\n",
    "experiment = \"experiment_\" + datetime.date.today().strftime(\"%Y%m%d\")\n",
    "prompts = \"data/airr_official_1.0_demo_en_us_prompt_set_release_reduced.csv\"\n",
    "ground_truth = \"data/fakegroundtruth.csv\"\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e0529-4e51-45ae-b2ab-313915881f98",
   "metadata": {},
   "source": [
    "## Defining a new annotator\n",
    "\n",
    "An implementation of the annotator should define:\n",
    "\n",
    "* `translate_request`: takes a `TestItem` and `SUTResponse` (which have the needed information on the prompt responses for the given LLM) and translates the request into an arbitrary form useful for the `annotate` method.\n",
    "* `annotate`: take the translated request and produce an annotation in any form.\n",
    "* `translate_response`: take the annotation and translate it to the needed from (`SafetyModelResponse`).\n",
    "\n",
    "Below, we give a sample `RandomAnnotator`, which randomly decides whether a given response is safe by hazard category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89674a6d-b2c5-42a3-9a0c-927101126877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from modelgauge.annotator import CompletionAnnotator\n",
    "from modelgauge.annotator_registry import ANNOTATORS\n",
    "from modelgauge.safety_model_response import SafetyModelResponse\n",
    "from modelgauge.single_turn_prompt_response import TestItem\n",
    "from modelgauge.sut import SUTResponse\n",
    "from modelgauge.tests.safe_v1 import BaseSafeTestVersion1\n",
    "\n",
    "class AnnotationRequest(BaseModel):\n",
    "    test_item: TestItem\n",
    "    response: SUTResponse\n",
    "\n",
    "\n",
    "class RandomAnnotator(CompletionAnnotator[SafetyModelResponse]):\n",
    "    \"\"\"Simple random implementation for demonstration. Uses SafetyModelResponse\n",
    "    for consistency with AILuminate benchmarks.\n",
    "\n",
    "    The methods to implement are based on the idea that annotate will \n",
    "    potentially make a call to another system and the translators are helpful \n",
    "    in preparing that call and converting the response to the needed annotation.\n",
    "    \"\"\"\n",
    "    def __init__(self, uid, prob_unsafe):\n",
    "        super().__init__(uid)\n",
    "        self.prob_unsafe = prob_unsafe\n",
    "    \n",
    "    def translate_request(self, test_item: TestItem, response: SUTResponse):\n",
    "        return AnnotationRequest(test_item=test_item, response=response)\n",
    "\n",
    "    def annotate(self, annotation_request: AnnotationRequest) -> SafetyModelResponse:\n",
    "        unsafe_categories = []\n",
    "        for category in BaseSafeTestVersion1.hazards:\n",
    "            if random.random() < self.prob_unsafe:\n",
    "                unsafe_categories.append(category)\n",
    "        return SafetyModelResponse(\n",
    "            is_safe=not unsafe_categories,\n",
    "            logprobs=random.random(),\n",
    "            safety_categories=unsafe_categories,\n",
    "        )\n",
    "\n",
    "    def translate_response(self, request: AnnotationRequest, response: SafetyModelResponse) -> SafetyModelResponse:\n",
    "        return response\n",
    "\n",
    "\n",
    "annotator_id = \"random001\"\n",
    "ANNOTATORS.register(RandomAnnotator, annotator_id, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17760cd3-23fe-4c79-8882-475d8d7096ea",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "This step will get responses to the prompts from the given SUT. You can optionally pass arguments `prompt_uid_col` and `prompt_text_col` if your prompts dataset has different column names than the default ones.\n",
    "\n",
    "Save this run_id to avoid having to re-run the model later. The results are saved as an artifact in mlflow.\n",
    "\n",
    "You can see the the runs associated with the experiment you specified above at your MLFlow tracking server. There will be a link below after running. You may need to replace http://mlflow:8080 with http://localhost:8080 in the generated mlflow links if you used the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d76d5-a3e1-4cc0-aeff-e71b6ff64825",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_run = responder.respond(\n",
    "    sut_id=sut_id,\n",
    "    experiment=experiment,\n",
    "    prompts=prompts,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a8a85-c171-4d11-b094-cd617b14b6ed",
   "metadata": {},
   "source": [
    "## Annotate the model\n",
    "\n",
    "This will run the evaluator on the responses from the prior step. You'll be able to see the details of the annotations in mlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06632c4d-90bd-4c2d-9c36-84e59dd8f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_run = annotator.annotate(\n",
    "    annotator_ids=[annotator_id],\n",
    "    experiment=experiment,\n",
    "    response_run_id=response_run.run_id,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f95d5-95ae-4919-96b0-55f2378d5846",
   "metadata": {},
   "source": [
    "## Score the model\n",
    "\n",
    "Compute metrics against the given ground truth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8672c-32b5-45b1-9be2-396ceb857f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer.score(\n",
    "    annotation_run_id=annotation_run.run_id,\n",
    "    experiment=experiment,\n",
    "    ground_truth=ground_truth,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9debec-28be-4a50-82da-5d7025de7d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
