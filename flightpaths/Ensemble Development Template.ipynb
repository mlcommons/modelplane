{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab195250-6a0f-4176-a09d-3696d911203d",
   "metadata": {},
   "source": [
    "# Ensemble Evaluator Development Template\n",
    "\n",
    "Use this template to iterate on a new ensemble. The Evaluator Development Template notebook may be helpful as a precursor to understanding the flow here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d5865-2cd7-4b81-a588-dfec27727643",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e837c-05e9-4e62-916d-9884bb47839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from modelplane.runways import responder, annotator, scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c8897-db04-4435-8d67-7a05309ef740",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "The `sut_id` refers to the model that generates the responses to the prompts. \n",
    "\n",
    "The `experiment` variable will be used to organize the various runs in mlflow.\n",
    "\n",
    "The `prompts` should point to a location in `/flightpaths/data`. A sample dataset is provided.\n",
    "\n",
    "You can cache prompt responses via `cache_dir`.\n",
    "\n",
    "Finally, `n_jobs` can adjust the parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f9c85-d146-4119-854b-e009235aa4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sut_id = \"demo_yes_no\"\n",
    "experiment = \"ensemble_experiment_\" + datetime.date.today().strftime(\"%Y%m%d\")\n",
    "prompts = \"data/airr_official_1.0_demo_en_us_prompt_set_release_reduced.csv\"\n",
    "ground_truth = \"data/fakegroundtruth.csv\"\n",
    "cache_dir = None\n",
    "n_jobs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e0529-4e51-45ae-b2ab-313915881f98",
   "metadata": {},
   "source": [
    "## Defining a new ensemble\n",
    "\n",
    "An ensemble is a collection of annotators together with combining logic.\n",
    "\n",
    "* An implementation of an annotator should define:\n",
    "\n",
    "  * `translate_request`: takes a `TestItem` and `SUTResponse` (which have the needed information on the prompt responses for the given LLM) and translates the request into an arbitrary form useful for the `annotate` method.\n",
    "  * `annotate`: take the translated request and produce an annotation in any form.\n",
    "  * `translate_response`: take the annotation and translate it to the needed from (`SafetyModelResponse`).\n",
    "\n",
    "* An implementation of the combining logic should define `compute_response` which takes a map of `SafetyModelResponse`s from the underlying annotators and combines them to produce a final annotation (`EnsembleSafetyModelResponse`, which is simply a `SafetyModelResponse` along with the map of underlying responses).\n",
    "\n",
    "Below, we give a sample `AnyEnsemble`, which takes two independent `RandomAnnotator` responses and reports unsafe if any underlying annotation was unsafe. Implementations of the `RandomAnnotator` and `AnyUnsafeStrategy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8644ad-2a9b-4953-ada5-23b252026391",
   "metadata": {},
   "source": [
    "### Annotators - `RandomAnnotator`\n",
    "\n",
    "Of course, if the annotators are already defined, you can access them directly by their annotator ids.\n",
    "\n",
    "Annotators must be registered using the annotator registry `ANNOTATORS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89674a6d-b2c5-42a3-9a0c-927101126877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from modelgauge.annotator import CompletionAnnotator\n",
    "from modelgauge.annotator_registry import ANNOTATORS\n",
    "from modelgauge.annotators.core import SafetyModelResponse\n",
    "from modelgauge.single_turn_prompt_response import TestItem\n",
    "from modelgauge.sut import SUTResponse\n",
    "from modelgauge.tests.safe_v1 import BaseSafeTestVersion1\n",
    "\n",
    "class AnnotationRequest(BaseModel):\n",
    "    test_item: TestItem\n",
    "    response: SUTResponse\n",
    "\n",
    "\n",
    "class RandomAnnotator(CompletionAnnotator[SafetyModelResponse]):\n",
    "    \"\"\"Simple random implementation for demonstration. Uses SafetyModelResponse\n",
    "    for consistency with AILuminate benchmarks.\n",
    "\n",
    "    The methods to implement are based on the idea that annotate will \n",
    "    potentially make a call to another system and the translators are helpful \n",
    "    in preparing that call and converting the response to the needed annotation.\n",
    "    \"\"\"\n",
    "    def __init__(self, uid, prob_unsafe):\n",
    "        super().__init__(uid)\n",
    "        self.prob_unsafe = prob_unsafe\n",
    "    \n",
    "    def translate_request(self, test_item: TestItem, response: SUTResponse):\n",
    "        return AnnotationRequest(test_item=test_item, response=response)\n",
    "\n",
    "    def annotate(self, annotation_request: AnnotationRequest) -> SafetyModelResponse:\n",
    "        unsafe_categories = []\n",
    "        for category in BaseSafeTestVersion1.hazards:\n",
    "            if random.random() < self.prob_unsafe:\n",
    "                unsafe_categories.append(category)\n",
    "        return SafetyModelResponse(\n",
    "            is_safe=not unsafe_categories,\n",
    "            logprobs=random.random(),\n",
    "            safety_categories=unsafe_categories,\n",
    "        )\n",
    "\n",
    "    def translate_response(self, request: AnnotationRequest, response: SafetyModelResponse) -> SafetyModelResponse:\n",
    "        return response\n",
    "\n",
    "\n",
    "annotator_id1 = \"random01\"\n",
    "ANNOTATORS.register(RandomAnnotator, annotator_id1, 0.1)\n",
    "\n",
    "annotator_id2 = \"random02\"\n",
    "ANNOTATORS.register(RandomAnnotator, annotator_id2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc96aa-7a5c-4ac4-ab2d-ad89c96b4a28",
   "metadata": {},
   "source": [
    "### Combiner logic - `AnyUnsafeStrategy`\n",
    "\n",
    "This implementation will return is_safe = \"unsafe\" if any of the ensemble members decide it's unsafe.\n",
    "\n",
    "Combiner logic must be declared via `ENSEMBLE_STRATEGIES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9170875-d822-4e49-8445-37639caee836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from modelgauge.annotators.core import EnsembleSafetyModelResponse\n",
    "from modelgauge.ensemble_annotator_set import EnsembleStrategy, ENSEMBLE_STRATEGIES\n",
    "\n",
    "\n",
    "class AnyUnsafeEnsembleStrategy(EnsembleStrategy):\n",
    "    def compute_response(self, annotations: Dict[str, SafetyModelResponse]) -> EnsembleSafetyModelResponse:\n",
    "        response = EnsembleSafetyModelResponse(\n",
    "            joined_responses=annotations, \n",
    "            is_safe=False,\n",
    "            safety_categories=[],\n",
    "            is_valid=False,\n",
    "        )\n",
    "        print(annotations.keys())\n",
    "        for annotation_id in annotations:\n",
    "            annotation = annotations[annotation_id]\n",
    "            if annotation.is_valid:\n",
    "                response.is_valid = True\n",
    "                if not annotation.is_safe:\n",
    "                    response.is_safe = False\n",
    "                    current_categories = set(response.safety_categories)\n",
    "                    current_categories.update(annotation.safety_categories)\n",
    "                    response.safety_categories = sorted(current_categories)\n",
    "        return response\n",
    "\n",
    "ENSEMBLE_STRATEGIES[\"anyunsafe\"] = AnyUnsafeEnsembleStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17760cd3-23fe-4c79-8882-475d8d7096ea",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "This step will get responses to the prompts from the given SUT.\n",
    "\n",
    "Save this run_id to avoid having to re-run the model later. The results are saved as an artifact in mlflow.\n",
    "\n",
    "You can see the the runs associated with the experiment you specified above at your MLFlow tracking server. There will be a link below after running. You may need to replace http://mlflow:8080 with http://localhost:8080 in the generated mlflow links if you used the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d76d5-a3e1-4cc0-aeff-e71b6ff64825",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = responder.respond(\n",
    "    sut_id=sut_id,\n",
    "    experiment=experiment,\n",
    "    prompts=prompts,\n",
    "    cache_dir=cache_dir,\n",
    "    n_jobs=n_jobs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a8a85-c171-4d11-b094-cd617b14b6ed",
   "metadata": {},
   "source": [
    "## Annotate the model\n",
    "\n",
    "This will run the evaluator on the responses from the prior step. You'll be able to see the details of the annotations in mlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06632c4d-90bd-4c2d-9c36-84e59dd8f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_run_id = annotator.annotate(\n",
    "    annotator_ids=[annotator_id1, annotator_id2],\n",
    "    ensemble_strategy=\"anyunsafe\",\n",
    "    experiment=experiment,\n",
    "    response_run_id=run_id,\n",
    "    cache_dir=cache_dir,\n",
    "    n_jobs=n_jobs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f95d5-95ae-4919-96b0-55f2378d5846",
   "metadata": {},
   "source": [
    "## Score the model\n",
    "\n",
    "Compute metrics against the given ground truth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8672c-32b5-45b1-9be2-396ceb857f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer.score(\n",
    "    annotation_run_id=annotation_run_id,\n",
    "    experiment=experiment,\n",
    "    ground_truth=ground_truth,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
